# Marxist Article Search Engine - Technical Design Document

---

## Executive Summary

A lightweight, cost-effective semantic search engine for a 25-year corpus of Marxist theoretical and analytical articles (~16,000 articles) sourced from the Revolutionary Communist International (RCI) and related organizations' RSS feeds. The system enables users to search across news content with intelligent query expansion and special term recognition for Marxist concepts, without the overhead of LLM inference costs.

**Key Technologies:** FastAPI, React, txtai, SQLite, trafilatura, feedparser, BAAI/bge-small-en-v1.5

**Deployment Target:** Single DigitalOcean droplet ($24/month, 4GB RAM, 2 vCPUs)

**Concurrency Model:** Single worker process with thread pool for efficient concurrent query handling

**Special Features:** 
- Automatic query expansion with Marxist terminology synonyms
- Bidirectional alias resolution (e.g., "USSR" ↔ "Soviet Union")
- Special term extraction and tracking for improved search relevance
- Search analytics tracking for optimization

---

## Project Goals

### Primary Objectives
1. **Semantic Search**: Enable natural language queries across 16,000+ articles dating back to 1998
2. **Marxist Context Awareness**: 
   - Intelligent query expansion for theoretical concepts (e.g., "proletariat" → "working class OR workers")
   - Bidirectional alias resolution (e.g., "UN" ↔ "United Nations", "USSR" ↔ "Soviet Union")
   - Special term recognition across 6 categories (people, organizations, concepts, geographic, historical events, movements)
3. **Cost Efficiency**: Eliminate LLM hosting costs by using local embeddings only
4. **Fast Search**: Sub-second query response times with concurrent user support
5. **Incremental Updates**: Automatically ingest new articles every 30 minutes without full reindexing
6. **Search Analytics**: Track term usage, synonym effectiveness, and search patterns for optimization

### Non-Goals
- No chatbot or conversational interface
- No answer generation or summarization
- No real-time streaming updates (30-min polling is sufficient)
- No multi-user authentication (public search interface)

---

## System Architecture

### High-Level Component Overview

```
┌─────────────────────────────────────────────────────────────┐
│                      Client Browser                         │
│              (React + Create React App + TailwindCSS)       │
└────────────────────────┬────────────────────────────────────┘
                         │ HTTP/REST
                         │
┌────────────────────────▼────────────────────────────────────┐
│                    Nginx (Reverse Proxy)                    │
│           • Serve React static files                        │
│           • Proxy /api/* to FastAPI                         │
│           • Rate limiting (100 req/min)                     │
└────────────────────────┬────────────────────────────────────┘
                         │
┌────────────────────────▼────────────────────────────────────┐
│              FastAPI Application (Single Worker)            │
│           • Async I/O for HTTP connections                  │
│           • Thread pool (4 threads) for CPU-bound searches  │
│           • Query expansion with term extractor             │
│           • Search endpoints with analytics tracking        │
│           • Filter endpoints (authors, sources)             │
└────────────────────────┬────────────────────────────────────┘
                         │
        ┌────────────────┼────────────────┐
        │                │                │
┌───────▼──────┐  ┌──────▼──────┐  ┌─────▼──────┐
│ txtai Index  │  │   SQLite    │  │ JSON Config│
│ (In-RAM)     │  │  Database   │  │   Files    │
│ Single Copy  │  │             │  │            │
│ Shared by    │  │ • Articles  │  │ • Terms    │
│ All Threads  │  │ • Chunks    │  │ • Analytics│
│              │  │ • Terms     │  │ • RSS Feeds│
│ ~2GB         │  │ ~200MB      │  │ ~1MB       │
└──────────────┘  └─────────────┘  └────────────┘
        ▲
        │
┌───────┴──────────────────────────────────────────────────────┐
│              Background Ingestion Service                    │
│         (Systemd Timer - Every 30 minutes)                   │
│                                                              │
│  1. Poll RSS feeds (feedparser) - parallel async             │
│  2. Stop after N consecutive duplicates (smart detection)   │
│  3. Extract full content (trafilatura)                       │
│  4. Extract tags from RSS feeds                              │
│  5. Extract special terms (pattern matching + categories)   │
│  6. Resolve aliases bidirectionally                          │
│  7. Chunk long articles (>3500 words)                        │
│  8. Generate embeddings (bge-small-en-v1.5)                  │
│  9. Update txtai index with upsert (no full rebuild)        │
│  10. Track analytics (search patterns, term hits)            │
└──────────────────────────────────────────────────────────────┘
```

---

## Technology Stack

### Backend
- **Framework**: FastAPI 
- **Implementation Language**: Python 3.11+
- **Web Server**: Uvicorn (single worker with async loop)
- **Reverse Proxy**: Nginx
- **Concurrency**: ThreadPoolExecutor (4 workers)

### Frontend
- **Framework**: React 18
- **Build Tool**: Create React App
- **Styling**: TailwindCSS 3
- **HTTP Client**: Fetch API (native)

### Search & Embeddings
- **Vector Search**: txtai 7.x
- **Embedding Model**: BAAI/bge-small-en-v1.5 (384 dimensions)
- **Hybrid Search**: txtai's built-in semantic (70%) + BM25 (30%)

### Data Storage
- **Database**: SQLite
- **Index Storage**: txtai (disk-backed, RAM-loaded)
- **Configuration**: JSON files

### Content Processing
- **RSS Parsing**: feedparser
- **Content Extraction**: trafilatura
- **Text Processing**: Custom normalization (TextNormalizer)
- **Special Term Extraction**: Pattern matching via TermExtractor

### CLI
- **Framework**: Click
- **Output Formatting**: Rich

---

## Special Term System (Core Feature)

### Overview

The special term system is a central feature that enhances search relevance through intelligent term extraction, synonym expansion, and bidirectional alias resolution. It consists of three main components:

1. **Term Extraction**: Identifies Marxist terminology in articles during indexing
2. **Query Expansion**: Automatically expands search queries with synonyms and aliases
3. **Analytics Tracking**: Monitors term usage and search patterns

### Term Configuration File (`terms_config.json`)

```json
{
  "synonyms": {
    "proletariat": ["working class", "workers", "wage laborers", "wage earners"],
    "bourgeoisie": ["capitalist class", "ruling class", "capitalists", "bosses"],
    "exploitation": ["surplus value", "unpaid labor", "expropriation"],
    "imperialism": ["imperialist powers", "colonial domination", "neocolonialism"],
    "revolution": ["socialist revolution", "revolutionary overthrow", "insurrection"]
  },
  "terms": {
    "people": [
      "Karl Marx", "Friedrich Engels", "Vladimir Lenin", "Leon Trotsky",
      "Rosa Luxemburg", "Ted Grant", "Alan Woods", "Jorge Martin"
    ],
    "organizations": [
      "IMT", "RCI", "RCA", "RCP", "Revolutionary Communist International",
      "NATO", "United Nations", "European Union", "IMF"
    ],
    "concepts": [
      "permanent revolution", "transitional program", "dialectical materialism",
      "historical materialism", "surplus value", "dictatorship of the proletariat"
    ],
    "geographic": [
      "Venezuela", "China", "Russia", "Cuba", "Britain", "USA",
      "Soviet Union", "Latin America", "Middle East"
    ],
    "historical_events": [
      "Russian Revolution", "October Revolution", "Spanish Civil War",
      "Chinese Revolution", "Cuban Revolution", "May 1968"
    ],
    "movements": [
      "labor movement", "trade union movement", "anti-war movement",
      "feminist movement", "climate movement"
    ]
  },
  "aliases": {
    "UN": "United Nations",
    "EU": "European Union",
    "IMF": "International Monetary Fund",
    "USSR": "Soviet Union",
    "USA": "United States",
    "UK": "Britain",
    "RCI": "Revolutionary Communist International",
    "IMT": "International Marxist Tendency"
  }
}
```

### Term Extraction Process

**Implementation**: `backend/src/ingestion/term_extractor.py`

**During article ingestion**, the TermExtractor:

1. **Loads configuration** from `terms_config.json`
2. **Builds lookup structures**:
   - `term_to_category`: Maps each term to its category
   - `compiled_patterns`: Regex patterns for whole-word matching
   - `alias_mapping`: Maps aliases to canonical terms (lowercase)
   - `reverse_alias_mapping`: Maps canonical terms to their aliases

3. **Extracts terms from articles**:
   - Searches title and content (title weighted 2x)
   - Uses case-insensitive whole-word matching
   - Counts occurrences per term
   - Resolves aliases to canonical terms

4. **Stores results**:
   - `articles.terms_json`: JSON array of all extracted terms
   - `term_mentions` table: Detailed term occurrences with counts and categories

**Example extraction**:
```python
# Article title: "USSR and the Permanent Revolution"
# Article content: "The Soviet Union's revolutionary history..."

# Extracted terms:
{
  "geographic": [
    {"term": "Soviet Union", "count": 5}  # From both "USSR" and "Soviet Union"
  ],
  "concepts": [
    {"term": "permanent revolution", "count": 3}
  ],
  "historical_events": [
    {"term": "Russian Revolution", "count": 2}
  ]
}
```

### Query Expansion Process

**Implementation**: `backend/src/search/search_engine.py` → `_expand_query()`

**During search**, queries are automatically expanded with synonyms and aliases:

#### Expansion Algorithm

1. **Multi-word canonical term matching**:
   - Check if query contains canonical terms like "Soviet Union"
   - If found, add all aliases: `"Soviet Union" → "(Soviet Union OR USSR)"`

2. **Word-by-word expansion**:
   - For each word in query:
     - Get synonyms from `terms_config.json`
     - Check if it's an alias (e.g., "USSR")
     - If alias, add canonical term: `"USSR" → "(USSR OR Soviet Union)"`
     - If base term with synonyms: `"proletariat" → "(proletariat OR working class OR workers OR wage laborers)"`

3. **Bidirectional resolution**:
   - `"USSR" → "(USSR OR Soviet Union)"` - user searches alias, gets canonical
   - `"Soviet Union" → "(Soviet Union OR USSR)"` - user searches canonical, gets alias

#### Expansion Examples

```
Original Query          →  Expanded Query
─────────────────────────────────────────────────────────────
"USSR collapse"         →  "(USSR OR Soviet Union) collapse"

"Soviet Union history"  →  "(Soviet Union OR USSR) history"

"proletariat struggle"  →  "(proletariat OR working class OR workers OR wage laborers) struggle"

"UN peacekeeping"       →  "(UN OR United Nations) peacekeeping"

"IMT organization"      →  "(IMT OR International Marxist Tendency) organization"
```

**Configuration**:
```python
# In search_engine.py
enable_query_expansion = True  # Toggle feature on/off
max_synonyms_per_term = 5      # Limit synonyms for performance
```

### Analytics Tracking

**Implementation**: `backend/src/search/analytics_tracker.py`

**Tracked Metrics**:

1. **Most Searched Terms** (by category):
   - people, organizations, concepts, geographic, historical_events, movements
   - Count of searches mentioning each term

2. **Term Hit Rates**:
   - Which terms from `terms_config.json` appear in top results
   - Tracks effectiveness of term extraction

3. **Most Searched Authors**:
   - Author popularity in search filters
   - Article counts per author

4. **Tag Distribution in Results**:
   - Which RSS feed tags appear most frequently
   - Helps optimize tag taxonomy

5. **Synonym Matching Stats**:
   - Total synonym expansions
   - Which synonyms get matched
   - Effectiveness metrics

6. **Search Volume by Date**:
   - Daily search counts
   - Trend analysis

7. **Searches with No Results**:
   - Tracks failed queries
   - Helps identify missing content or term gaps

**Storage**: `backend/config/analytics_config.json`

**Update Frequency**: Every 100 searches (configurable)

**Analytics JSON Structure**:
```json
{
  "tracking": {
    "most_searched_terms": {
      "people": {
        "Karl Marx": 245,
        "Lenin": 189
      },
      "organizations": {
        "United Nations": 156,
        "NATO": 98
      }
    },
    "term_hit_rates": {
      "concepts": {
        "permanent revolution": 342,
        "dialectical materialism": 287
      }
    },
    "synonym_matching_stats": {
      "total_synonym_matches": 1247,
      "matches_by_term": {
        "proletariat": {
          "working class": 456,
          "workers": 234
        }
      }
    }
  },
  "metadata": {
    "last_updated": "2025-11-18T19:30:00Z",
    "total_searches_tracked": 5432
  }
}
```

### Benefits of Special Term System

1. **Improved Search Recall**: Users searching "USSR" automatically get "Soviet Union" results
2. **Theoretical Precision**: Marxist concepts properly linked through synonyms
3. **User Flexibility**: Works with abbreviations, full names, or theoretical terms
4. **Analytics Insights**: Understand what users search for most
5. **Content Optimization**: Identify gaps in article coverage

---

## Data Architecture

### Database Schema (SQLite)

#### Articles Table
```sql
CREATE TABLE articles (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    url TEXT UNIQUE NOT NULL,
    guid TEXT UNIQUE,
    title TEXT NOT NULL,
    content TEXT NOT NULL,            -- Full extracted article text
    summary TEXT,                     -- RSS feed summary
    source TEXT NOT NULL,             -- RSS feed name
    author TEXT,
    published_date DATETIME NOT NULL,
    fetched_date DATETIME NOT NULL,
    word_count INTEGER,
    is_chunked BOOLEAN DEFAULT 0,
    indexed BOOLEAN DEFAULT 0,
    embedding_version TEXT DEFAULT '1.0',
    terms_json TEXT,                  -- JSON array of extracted special terms
    tags_json TEXT                    -- JSON array of RSS tags/categories
);

CREATE INDEX idx_source ON articles(source);
CREATE INDEX idx_published_date ON articles(published_date);
CREATE INDEX idx_author ON articles(author);
CREATE INDEX idx_indexed ON articles(indexed);
```

#### Article Chunks Table
```sql
CREATE TABLE article_chunks (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    article_id INTEGER NOT NULL,
    chunk_index INTEGER NOT NULL,    -- Order within article (0, 1, 2...)
    content TEXT NOT NULL,
    word_count INTEGER,
    start_position INTEGER,          -- Character position in original
    FOREIGN KEY (article_id) REFERENCES articles(id),
    UNIQUE(article_id, chunk_index)
);
```

#### Author Statistics Table
```sql
CREATE TABLE author_stats (
    author TEXT PRIMARY KEY,
    article_count INTEGER DEFAULT 0,
    latest_article_date DATETIME,
    first_article_date DATETIME
);
```

#### RSS Feeds Table
```sql
CREATE TABLE rss_feeds (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    url TEXT UNIQUE NOT NULL,
    name TEXT NOT NULL,
    pagination_type TEXT DEFAULT 'standard',  -- 'wordpress', 'joomla', 'standard'
    limit_increment INTEGER DEFAULT 5,
    last_checked DATETIME,
    last_modified DATETIME,          -- For If-Modified-Since headers
    etag TEXT,                        -- For HTTP caching
    status TEXT DEFAULT 'active',    -- 'active', 'degraded', 'failing'
    consecutive_failures INTEGER DEFAULT 0,
    article_count INTEGER DEFAULT 0
);
```

#### Special Term Mentions Table
```sql
CREATE TABLE term_mentions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    article_id INTEGER NOT NULL,
    term_text TEXT NOT NULL,
    term_type TEXT,                  -- Category: 'people', 'organizations', 'concepts', etc.
    mention_count INTEGER DEFAULT 1,
    FOREIGN KEY (article_id) REFERENCES articles(id)
);

CREATE INDEX idx_term_text ON term_mentions(term_text);
```

#### Search Logs Table (Optional)
```sql
CREATE TABLE search_logs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    query TEXT NOT NULL,
    filters_json TEXT,
    result_count INTEGER,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
);
```

---

## txtai Configuration

### Index Configuration (txtai 7.x)
```python
{
    # Embedding model
    "path": "BAAI/bge-small-en-v1.5",
    
    # Store content with embeddings in SQLite
    "content": True,
    
    # Enable keyword search alongside semantic
    "keyword": True,
    
    # Metadata columns stored in txtai's internal SQLite
    # (Note: In txtai 7.x, all fields are automatically stored)
    "columns": {
        "id": "INTEGER PRIMARY KEY",
        "article_id": "INTEGER",
        "title": "TEXT",
        "url": "TEXT",
        "source": "TEXT",
        "author": "TEXT",
        "published_date": "DATETIME",
        "published_year": "INTEGER",
        "published_month": "INTEGER",
        "word_count": "INTEGER",
        "is_chunk": "BOOLEAN",
        "chunk_index": "INTEGER",
        "terms": "TEXT",                # JSON string of extracted terms
        "tags": "TEXT"                  # JSON string of RSS tags
    },

    # Use numpy backend instead of faiss to avoid nflip AttributeError
    # numpy provides CPU-only exact search without requiring additional dependencies
    "backend": "numpy"
}
```

### Embedding Model Details
- **Model**: BAAI/bge-small-en-v1.5
- **Dimensions**: 384
- **Size**: ~133MB
- **Speed**: ~4-5x faster than larger models
- **Initial Indexing Time**: 3-5 hours for 16k articles (CPU)
- **Incremental Update**: < 1 minute for 10-50 new articles

---

## Content Processing Pipeline

### 1. RSS Feed Polling

**Frequency**: Every 30 minutes via systemd timer

**Implementation**: `backend/src/ingestion/rss_fetcher.py`

**Features**:
- Parallel async fetching of all configured feeds
- Pagination support for WordPress, Joomla, and standard RSS
- HTTP caching with ETag and Last-Modified headers
- Deduplication by URL and GUID

**Pagination Types**:

```python
# WordPress: ?paged=N
"https://example.com/feed/?paged=2"

# Joomla: ?format=feed&limitstart=N
"https://example.com/index.php?format=feed&limitstart=10"

# Standard: Check for <link rel="next">
# No additional parameters
```

### 2. Incremental Update Logic (Smart Duplicate Detection)

**Implementation**: `backend/src/ingestion/archiving_service.py` → `update_feeds()`

**Key Innovation**: Stops fetching after N consecutive duplicates

**Algorithm**:
1. Load all existing article URLs from database into memory (set)
2. For each RSS feed:
   - Fetch RSS entries (newest first)
   - For each entry:
     - Check if URL exists in database
     - If NEW: Add to extraction queue, reset duplicate counter
     - If DUPLICATE: Increment consecutive duplicate counter
     - If consecutive duplicates >= 5: STOP fetching this feed
   - Extract and save only new articles

**Why It Works**:
- RSS feeds are ordered newest-first
- Once we hit 5 articles we've already indexed, we know we've reached old content
- Saves 90%+ of fetch time compared to full archive
- Typical update: 0-50 new articles, completes in < 5 minutes

**Configuration**:
```python
# In incremental_update.py
max_consecutive_duplicates = 5  # Stop after 5 dupes
```

### 3. Content Extraction

**Implementation**: `backend/src/ingestion/content_extractor.py`

**Two-Stage Strategy**:

1. **Parse RSS with feedparser**:
   - Get metadata: title, author, published date, GUID
   - Get summary/description
   - Get tags/categories
   - Check if full content included (>200 chars)

2. **Extract full text with trafilatura** (if needed):
   - Only if RSS content < 200 chars (summary only)
   - Fetch article URL
   - Extract main content (removes ads, nav, etc.)
   - Timeout: 30 seconds per article

**Normalization**: `backend/src/ingestion/text_normalizer.py`
- Decode HTML entities
- Remove HTML tags
- Normalize whitespace
- Preserve paragraph breaks
- Redact email addresses

### 4. Special Term Extraction

**Implementation**: `backend/src/ingestion/term_extractor.py`

**Process**:
1. Load `terms_config.json` on initialization
2. Build regex patterns for each term (whole-word matching)
3. Build alias mappings (bidirectional)
4. For each article:
   - Search title and content (title weighted 2x)
   - Count occurrences per term
   - Resolve aliases to canonical terms
   - Categorize by term type
5. Store results:
   - `articles.terms_json`: JSON array of all terms
   - `term_mentions` table: Detailed term data

**Performance**:
- Pre-compiled regex patterns (fast)
- Case-insensitive matching
- ~100+ terms matched per article in ~10ms

### 5. Article Chunking

**Implementation**: `backend/src/indexing/chunking.py`

**Parameters**:
- **Chunk Threshold**: 3,500 words
- **Chunk Size**: 1,000 words
- **Overlap**: 200 words
- **Method**: Paragraph-boundary chunking

**Logic**:
1. Check if article > 3,500 words
2. If yes, split by paragraphs (`\n\n`)
3. Group paragraphs into ~1,000-word chunks
4. Maintain 200-word overlap between chunks
5. Store in `article_chunks` table
6. Set `is_chunked=1` on parent article
7. Each chunk gets its own embedding

**Chunk Metadata in Index**:
- Each chunk is a separate document in txtai
- Metadata includes `article_id` for deduplication
- `is_chunk` flag and `chunk_index` for tracking
- Search deduplicates by `article_id`, keeping highest score

### 6. Embedding Generation

**Model**: BAAI/bge-small-en-v1.5

**Process**:
1. For each article/chunk:
   - Create document dict with metadata
   - Generate 384-dim embedding
   - Store in txtai index with metadata
2. Index supports both:
   - New index creation (`index()`)
   - Incremental updates (`upsert()`)

**Incremental Updates**:
- Only embed new/unindexed articles (`indexed=0`)
- Use txtai's `upsert()` method (no full rebuild)
- Mark articles as `indexed=1` after completion
- Brief write lock (~100ms) during upsert

---

## Search Implementation

### Query Processing Flow

```
User Query: "USSR collapse"
    │
    ├─→ Query Expansion (TermExtractor)
    │   └─→ "(USSR OR Soviet Union) collapse"
    │
    ├─→ Build WHERE Clause (filters)
    │   └─→ "source = 'X' AND published_year >= 2020"
    │
    ├─→ Execute txtai Search (hybrid semantic + BM25)
    │   └─→ Raw results (may include multiple chunks per article)
    │
    ├─→ Deduplicate (group by article_id)
    │   └─→ Keep highest-scoring chunk per article
    │   └─→ Track matched_sections count
    │
    ├─→ Apply Recency Boost
    │   └─→ +0.05 if < 30 days old
    │
    ├─→ Format Results
    │   └─→ Create excerpts, parse JSON fields
    │
    ├─→ Track Analytics
    │   └─→ Update analytics_config.json
    │
    └─→ Return JSON Response
```

### Async Search Architecture

**Implementation**: `backend/src/api/routes.py`

```python
# Global thread pool for CPU-bound operations
search_executor = ThreadPoolExecutor(max_workers=4)

@app.post("/api/v1/search")
async def search_articles(request: SearchRequest):
    """
    Async endpoint: handles multiple concurrent requests efficiently
    Offloads CPU-bound txtai search to thread pool
    """
    # Runs in event loop (non-blocking I/O)
    result = await asyncio.get_event_loop().run_in_executor(
        search_executor,
        perform_search,  # Synchronous function runs in thread
        request.query,
        request.filters
    )
    return result

def perform_search(query: str, filters: dict):
    """
    Synchronous CPU-bound search function
    Multiple threads can call this simultaneously
    """
    # Thread-safe read on shared index
    results = search_engine.search(query, filters)
    return results
```

### Thread-Safe Index Management

**Implementation**: `backend/src/search/search_engine.py`

```python
import threading

class SearchEngine:
    def __init__(self):
        self.embeddings = None
        self.rw_lock = threading.RLock()
        self.term_extractor = TermExtractor(TERMS_CONFIG)
    
    def load_index(self):
        """Load index once on startup"""
        self.embeddings = Embeddings()
        self.embeddings.load(self.index_path)
    
    def search(self, query, filters):
        """
        Thread-safe read operation
        Multiple threads can search simultaneously
        No lock needed - reads don't modify index
        """
        # 1. Expand query with synonyms/aliases
        expanded_query = self._expand_query(query)
        
        # 2. Execute txtai search
        results = self._execute_txtai_search(expanded_query, filters)
        
        # 3. Deduplicate chunks
        deduplicated = self._deduplicate_results(results)
        
        # 4. Apply recency boost
        boosted = self._apply_recency_boost(deduplicated)
        
        # 5. Format results
        formatted = self._format_results(boosted, query)
        
        return formatted
    
    def _expand_query(self, query: str) -> str:
        """
        Expand query with synonyms and bidirectional aliases
        
        Examples:
          "USSR" → "(USSR OR Soviet Union)"
          "Soviet Union" → "(Soviet Union OR USSR)"
          "proletariat" → "(proletariat OR working class OR workers)"
        """
        if not self.term_extractor:
            return query
        
        # Handle multi-word canonical terms
        for canonical, aliases in self.term_extractor.reverse_alias_mapping.items():
            if canonical in query.lower():
                variants = [canonical] + aliases
                variant_clause = " OR ".join(f'"{v}"' for v in variants)
                query = query.replace(canonical, f"({variant_clause})")
        
        # Word-by-word expansion
        words = query.split()
        expanded_parts = []
        
        for word in words:
            if '(' in word:  # Already expanded
                expanded_parts.append(word)
                continue
            
            clean_word = word.strip('.,!?;:')
            
            # Get synonyms
            synonyms = self.term_extractor.get_synonyms_for_query(clean_word)
            
            # Check for alias
            alias_match = self.term_extractor.alias_mapping.get(clean_word.lower())
            if alias_match:
                canonical = self.term_extractor._get_original_term(alias_match)
                synonyms_set = set(synonyms + [canonical, clean_word])
                synonyms = list(synonyms_set)
            
            if len(synonyms) > 1:
                # Create OR clause (limit to 5 for performance)
                synonym_clause = " OR ".join(f'"{s}"' for s in synonyms[:5])
                expanded_parts.append(f"({synonym_clause})")
            else:
                expanded_parts.append(word)
        
        return " ".join(expanded_parts)
```

### Search Scoring Formula

```
final_score = (0.7 × semantic_similarity) + (0.3 × BM25_score) + recency_boost

recency_boost = {
  0.05  if article age < 30 days
  0.02  if article age < 90 days
  0.01  if article age < 1 year
  0.0   otherwise
}
```

### Deduplication Logic

**Why Needed**: Long articles are chunked, so multiple chunks from same article may match

**Process**:
1. Group all results by `article_id`
2. For each article:
   - Keep only the highest-scoring chunk
   - Add `matched_sections: N` metadata (N = number of matching chunks)
3. Return article-level results (not chunk-specific)

**Example**:
```
Raw Results (from txtai):
  - Article 123, Chunk 0: score=0.85
  - Article 123, Chunk 2: score=0.78
  - Article 456, Chunk 0: score=0.92

Deduplicated Results:
  - Article 123: score=0.85, matched_sections=2
  - Article 456: score=0.92, matched_sections=1
```

### Filter Implementation

**Implementation**: `backend/src/search/filters.py`

#### Date Filters

**UI Presets** (mapped to SQL):
```python
date_range_mapping = {
    "past_week": "published_date >= '2025-11-11'",
    "past_month": "published_date >= '2025-10-18'",
    "past_3months": "published_date >= '2025-08-18'",
    "past_year": "published_date >= '2024-11-18'",
    "2020s": "published_year >= 2020 AND published_year <= 2029",
    "2010s": "published_year >= 2010 AND published_year <= 2019",
    "2000s": "published_year >= 2000 AND published_year <= 2009",
    "1990s": "published_year >= 1990 AND published_year <= 1999"
}
```

**Custom Date Range**:
```python
# User selects start/end dates
filters = {
    "start_date": "2020-01-01",
    "end_date": "2025-12-31"
}

# Converted to SQL
where_clause = "published_date BETWEEN '2020-01-01' AND '2025-12-31'"
```

#### Combined Filters Example

```sql
-- Search with multiple filters
SELECT * FROM txtai 
WHERE similar('(proletariat OR working class) struggle', 0.7)
  AND source = 'In Defence of Marxism'
  AND author = 'Alan Woods'
  AND published_year >= 2020
LIMIT 50
```

---

## API Specification

### Base URL
```
https://yourdomain.com/api/v1
```

### Endpoints

#### POST /api/v1/search
Execute semantic search with filters and automatic query expansion.

**Request Body**:
```json
{
  "query": "USSR collapse",
  "filters": {
    "source": "In Defence of Marxism",
    "author": "Alan Woods",
    "date_range": "2020s",
    "limit": 50,
    "offset": 0
  }
}
```

**Response**:
```json
{
  "results": [
    {
      "id": 12345,
      "article_id": 12345,
      "title": "The Collapse of the Soviet Union: Lessons for Today",
      "url": "https://marxist.com/article/123",
      "source": "In Defence of Marxism",
      "author": "Alan Woods",
      "published_date": "2023-03-15T10:30:00Z",
      "excerpt": "The collapse of the USSR was not inevitable...",
      "score": 0.8523,
      "matched_sections": 2,
      "word_count": 4500,
      "tags": ["history", "soviet union", "collapse"],
      "terms": ["Soviet Union", "USSR", "Lenin", "Bolsheviks"],
      "recency_boost": 0.01,
      "original_score": 0.8423
    }
  ],
  "total": 127,
  "page": 1,
  "limit": 50,
  "offset": 0,
  "query_time_ms": 45,
  "query": "(USSR OR Soviet Union) collapse",
  "original_query": "USSR collapse",
  "query_expanded": true,
  "filters": {...}
}
```

**Query Expansion Fields**:
- `query`: Actual expanded query used for search
- `original_query`: User's original input
- `query_expanded`: Boolean indicating if expansion occurred

#### GET /api/v1/top-authors
Get top 15 authors by article count.

**Query Parameters**:
- `min_articles` (optional, default: 10) - Minimum article count
- `limit` (optional, default: 15) - Maximum authors to return

**Response**:
```json
{
  "authors": [
    {
      "name": "Alan Woods",
      "article_count": 847,
      "latest_article": "2025-11-15T08:00:00Z",
      "earliest_article": "2005-03-12T00:00:00Z"
    }
  ],
  "total": 15
}
```

#### GET /api/v1/sources
List all RSS feed sources.

**Response**:
```json
{
  "sources": [
    {
      "name": "In Defence of Marxism",
      "article_count": 5234,
      "latest_article": "2025-11-18T19:30:00Z",
      "earliest_article": "1998-06-15T00:00:00Z"
    }
  ],
  "total": 3
}
```

#### GET /api/v1/stats
Get index and database statistics.

**Response**:
```json
{
  "total_articles": 16234,
  "indexed_articles": 16234,
  "total_chunks": 2341,
  "date_range": {
    "earliest": "1998-06-15T00:00:00Z",
    "latest": "2025-11-18T19:30:00Z"
  },
  "sources_count": 3,
  "index_document_count": 18575,
  "index_loaded": true
}
```

#### GET /api/v1/health
Health check endpoint for monitoring.

**Response**:
```json
{
  "status": "healthy",
  "index_loaded": true,
  "index_document_count": 18575,
  "database_connected": true,
  "uptime_seconds": 86400
}
```

---

## Frontend Specification

### Technology Stack
- React 18
- Create React App (build tool)
- TailwindCSS 3 (styling)
- Fetch API (HTTP client)

### Component Structure

```
frontend/src/
├── components/
│   ├── SearchBar.jsx           # Main search input with debouncing
│   ├── FilterPanel.jsx         # Source/author/date filters
│   ├── ResultsList.jsx         # Results container with loading/error states
│   ├── ResultCard.jsx          # Individual result display
│   ├── Pagination.jsx          # Page navigation with configurable size
│   └── StatsDisplay.jsx        # Index statistics widget
├── hooks/
│   ├── useSearch.js            # Search state management + debouncing
│   └── useFilters.js           # Filter state management
├── utils/
│   └── api.js                  # API client with error handling
├── App.js                      # Main app component
└── index.js                    # Entry point
```

### Search Interface Layout

```
┌───────────────────────────────────────────────────────────────┐
│  Marxist Article Search                                      │
│  Search publications from across the Revolutionary Communist │
│  International                                                │
└───────────────────────────────────────────────────────────────┘

┌───────────────────────────────────────────────────────────────┐
│  Statistics Display                                           │
│  Total: 16,234 | Sources: 3 | Earliest: 1998 | Latest: 2025  │
└───────────────────────────────────────────────────────────────┘

┌───────────────────────────────────────────────────────────────┐
│  [          Search 16,000+ articles...             ] [Search] │
└───────────────────────────────────────────────────────────────┘

┌───────────────────────────────────────────────────────────────┐
│  Filters                                                      │
│  Source: [All Sources ▼]                                     │
│  Author: [All Authors ▼]                                     │
│  Date:   [Any Time ▼] or [Custom range...]                  │
└───────────────────────────────────────────────────────────────┘

┌───────────────────────────────────────────────────────────────┐
│  127 articles found • Query time: 45ms                        │
│                                                               │
│  ┌──────────────────────────────────────────────────────┐   │
│  │ Article Title                                         │   │
│  │ In Defence of Marxism • Alan Woods • Mar 15, 2025   │   │
│  │ Article excerpt with matched terms...                 │   │
│  │ [Read Article →]  2 sections matched  Score: 0.85    │   │
│  └──────────────────────────────────────────────────────┘   │
│                                                               │
│  Results per page: [25 ▼]                                    │
│  [← Previous]  1 2 3 4 5 ... 13  [Next →]                   │
└───────────────────────────────────────────────────────────────┘
```

### Custom Hooks

#### useSearch.js
```javascript
export const useSearch = () => {
  const [query, setQuery] = useState('');
  const [results, setResults] = useState([]);
  const [loading, setLoading] = useState(false);
  
  const debouncedSearch = useCallback((searchQuery, filters, immediate) => {
    // Debounce logic (300ms)
    // Offload to API
  }, []);
  
  return { query, setQuery, results, loading, debouncedSearch };
};
```

#### useFilters.js
```javascript
export const useFilters = () => {
  const [filters, setFilters] = useState({
    source: '',
    author: '',
    dateRange: '',
    limit: 25,
    offset: 0
  });
  
  const buildApiFilters = useCallback(() => {
    // Convert UI filters to API format
  }, [filters]);
  
  return { filters, updateFilter, clearFilters, buildApiFilters };
};
```

---

## CLI Tool Specification

### Commands

**Implementation**: `backend/src/cli/marxist_cli.py`

```bash
# Initialize database
python -m src.cli.marxist_cli init-db

# Archive all feeds (initial setup)
python -m src.cli.marxist_cli archive run

# Incremental update (recommended for regular use)
python -m src.cli.marxist_cli archive update

# List configured feeds
python -m src.cli.marxist_cli archive list

# Build initial index
python -m src.cli.marxist_cli index build

# Update index with new articles (after archive update)
python -m src.cli.marxist_cli index update

# View index info
python -m src.cli.marxist_cli index info

# Search from CLI
python -m src.cli.marxist_cli search "USSR collapse" \
  --source "In Defence of Marxism" \
  --date-range past_year \
  --limit 10

# View statistics
python -m src.cli.marxist_cli stats
```

### Rich Output

Uses Rich library for colorized, formatted CLI output:
- Progress bars during archiving
- Tables for statistics
- Colored status messages
- Formatted search results

---

## Deployment Specification

### Infrastructure

**Target**: Single DigitalOcean Droplet
- **Plan**: $24/month
- **Specs**: 4GB RAM, 2 vCPUs, 80GB SSD
- **OS**: Ubuntu 24.04 LTS

### File System Structure

```
/opt/marxist-search/            # Application code
├── backend/
│   ├── src/
│   ├── config/
│   │   ├── rss_feeds.json
│   │   ├── terms_config.json
│   │   ├── analytics_config.json
│   │   └── search_config.py
│   ├── requirements.txt
│   └── venv/
├── frontend/
│   └── build/                  # Built React app
└── scripts/
    ├── backup_db.sh
    └── backup_index.sh

/var/lib/marxist-search/        # Data directory
├── articles.db                 # SQLite database (~200MB)
├── txtai/                      # Index files (~2GB)
│   ├── embeddings/
│   ├── documents/
│   └── config/
└── cache/                      # JSON cache files

/var/log/news-search/           # Logs
├── api.log
├── ingestion.log
├── search.log
└── errors.log

/var/backups/marxist-search/    # Backups
├── daily/
│   └── articles_YYYY-MM-DD.db
└── weekly/
    └── full_YYYY-MM-DD.tar.gz
```

### Systemd Services

#### 1. API Service
**File**: `/etc/systemd/system/marxist-search-api.service`

```ini
[Unit]
Description=Marxist Search API
After=network.target

[Service]
Type=exec
User=newsearch
Group=newsearch
WorkingDirectory=/opt/marxist-search/backend
Environment="PATH=/opt/marxist-search/venv/bin"
ExecStart=/opt/marxist-search/venv/bin/python -m src.api.main
Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
```

#### 2. Incremental Update Timer
**File**: `/etc/systemd/system/marxist-search-update.timer`

```ini
[Unit]
Description=Marxist Search Update Timer
Requires=marxist-search-update.service

[Timer]
OnBootSec=5min
OnUnitActiveSec=30min
Unit=marxist-search-update.service

[Install]
WantedBy=timers.target
```

**File**: `/etc/systemd/system/marxist-search-update.service`

```ini
[Unit]
Description=Marxist Search Incremental Update

[Service]
Type=oneshot
User=newsearch
Group=newsearch
WorkingDirectory=/opt/marxist-search/backend
Environment="PATH=/opt/marxist-search/venv/bin"
ExecStart=/opt/marxist-search/venv/bin/python -m src.scripts.incremental_update
StandardOutput=append:/var/log/news-search/ingestion.log
StandardError=append:/var/log/news-search/errors.log
```

**Incremental Update Script**: `backend/src/scripts/incremental_update.py`
- Runs archiving update (smart duplicate detection)
- Updates txtai index with new articles
- Logs all operations
- Typically completes in < 5 minutes

#### 3. Nginx Configuration
**File**: `/etc/nginx/sites-available/marxist-search`

```nginx
upstream marxist_search_backend {
    server 127.0.0.1:8000;
    keepalive 32;
}

limit_req_zone $binary_remote_addr zone=api:10m rate=100r/m;

server {
    listen 80;
    server_name yourdomain.com;

    # React static files
    location / {
        root /opt/marxist-search/frontend/build;
        try_files $uri $uri/ /index.html;
        
        # Cache static assets
        location ~* \.(js|css|png|jpg|jpeg|gif|ico|svg|woff|woff2)$ {
            expires 1y;
            add_header Cache-Control "public, immutable";
        }
    }

    # API proxy
    location /api/ {
        limit_req zone=api burst=20 nodelay;
        
        proxy_pass http://marxist_search_backend/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_http_version 1.1;
        proxy_set_header Connection "";
    }
}
```

---

## Concurrency & Performance

### Concurrency Model

**Single Worker + Thread Pool**:
- 1 Uvicorn worker process
- 4 search threads in ThreadPoolExecutor
- Shared txtai index (2GB in RAM)
- Total memory: ~3.4GB

### Performance Targets

| Metric | Target | Notes |
|--------|--------|-------|
| Single query | < 100ms | 95th percentile |
| Concurrent (2-4) | < 150ms | Parallel execution |
| Concurrent (5-10) | < 300ms | Thread pool saturated |
| Throughput | 200-300/min | Sustained load |
| Initial indexing | 3-5 hours | One-time, can run offline |
| Incremental update | < 5 minutes | Typical 0-50 new articles |
| Index update (upsert) | < 1 minute | No full rebuild |

### Memory Usage

```
Component              Memory
─────────────────────────────
txtai index            ~2.0 GB
Python runtime         ~300 MB
SQLite cache           ~100 MB
FastAPI app            ~200 MB
OS + overhead          ~800 MB
─────────────────────────────
Total                  ~3.4 GB
Available (4GB)        ~600 MB
```

---

## Monitoring & Analytics

### Search Analytics (`analytics_config.json`)

**Tracked Metrics**:
1. Most searched terms (by category)
2. Term hit rates (which terms appear in results)
3. Author search popularity
4. Tag distribution in results
5. Synonym matching effectiveness
6. Search volume trends
7. Searches with no results

**Usage**:
- Identify popular topics
- Optimize `terms_config.json`
- Discover content gaps
- Improve synonym mappings

### Logging

**Log Files** (`/var/log/news-search/`):
- `api.log`: API requests, query performance
- `ingestion.log`: RSS fetching, article processing
- `search.log`: Search queries, analytics
- `errors.log`: All errors

**Format**: JSON Lines for easy parsing

### Health Monitoring

**Endpoint**: `GET /api/v1/health`

**Metrics**:
- Index loaded status
- Document count
- Database connectivity
- Uptime
- Thread pool utilization

---

## Security & Best Practices

### Input Validation
- Sanitize all search queries
- Validate filter parameters
- Limit query length (500 chars)
- Escape special characters

### Rate Limiting
- Nginx: 100 requests/minute per IP
- Burst: 20 requests
- Application-level backup (SlowAPI)

### Data Privacy
- No user authentication required
- No PII collection
- Anonymous search logs
- Respect RSS feed terms of service

### HTTPS/SSL
- Let's Encrypt for free SSL
- Automatic renewal via certbot
- Redirect HTTP to HTTPS

---

## Initial Setup Checklist

### One-Time Setup

**Infrastructure**:
- [ ] Provision DigitalOcean droplet
- [ ] Configure SSH, firewall (ports 22, 80, 443)
- [ ] Install dependencies (Python 3.11+, Nginx, Git)

**Application**:
- [ ] Clone repository to `/opt/marxist-search/`
- [ ] Create virtual environment
- [ ] Install Python dependencies
- [ ] Create application user

**Configuration**:
- [ ] Configure `rss_feeds.json` with feed URLs
- [ ] Configure `terms_config.json` with Marxist terminology
- [ ] Initialize `analytics_config.json`
- [ ] Set up `.env` file

**Initial Indexing** (3-5 hours):
- [ ] Run: `python -m src.cli.marxist_cli init-db`
- [ ] Run: `python -m src.cli.marxist_cli archive run`
- [ ] Run: `python -m src.cli.marxist_cli index build`
- [ ] Verify database and index created

**System Services**:
- [ ] Configure systemd services
- [ ] Enable and start API service
- [ ] Enable and start update timer
- [ ] Check logs for errors

**Web Server**:
- [ ] Configure Nginx
- [ ] Set up SSL certificate
- [ ] Test HTTPS access

**Frontend**:
- [ ] Build React app: `npm run build`
- [ ] Deploy to Nginx directory
- [ ] Test web interface

**Monitoring**:
- [ ] Set up log rotation
- [ ] Configure backups (daily DB, weekly full)
- [ ] Test backup/restore

---

## Appendix: Key Configuration Files

### terms_config.json (excerpt)
```json
{
  "synonyms": {
    "proletariat": ["working class", "workers", "wage laborers"],
    "bourgeoisie": ["capitalist class", "ruling class", "capitalists"],
    "imperialism": ["imperialist powers", "colonial domination"]
  },
  "terms": {
    "people": ["Karl Marx", "Lenin", "Trotsky", "Alan Woods"],
    "organizations": ["RCI", "IMT", "UN", "NATO"],
    "concepts": ["permanent revolution", "dialectical materialism"],
    "geographic": ["USSR", "Soviet Union", "China", "Cuba"],
    "historical_events": ["Russian Revolution", "October Revolution"],
    "movements": ["labor movement", "climate movement"]
  },
  "aliases": {
    "UN": "United Nations",
    "USSR": "Soviet Union",
    "IMT": "International Marxist Tendency"
  }
}
```

### rss_feeds.json
```json
{
  "feeds": [
    {
      "name": "In Defence of Marxism",
      "url": "https://marxist.com/index.php?format=feed",
      "pagination_type": "joomla",
      "limit_increment": 5,
      "enabled": true,
      "organization": "RCI"
    },
    {
      "name": "Revolutionary Communist Party",
      "url": "https://communist.red/feed",
      "pagination_type": "wordpress",
      "enabled": true,
      "organization": "RCP-UK"
    },
    {
      "name": "Revolutionary Communists of America",
      "url": "https://communistusa.org/feed",
      "pagination_type": "wordpress",
      "enabled": true,
      "organization": "RCA"
    }
  ]
}
```

### search_config.py (excerpt)
```python
# txtai Configuration
TXTAI_CONFIG = {
    "path": "BAAI/bge-small-en-v1.5",
    "content": True,
    "keyword": True,
    # Use numpy backend instead of faiss to avoid nflip AttributeError
    # numpy provides CPU-only exact search without requiring additional dependencies
    "backend": "numpy"
}

# Search Configuration
SEARCH_CONFIG = {
    "semantic_weight": 0.7,
    "bm25_weight": 0.3,
    "recency_boost": {
        "30_days": 0.05,
        "90_days": 0.02,
        "1_year": 0.01
    }
}

# Concurrency Configuration
CONCURRENCY_CONFIG = {
    "uvicorn_workers": 1,
    "search_thread_pool_size": 4,
    "max_concurrent_searches": 10,
    "search_timeout_seconds": 5.0
}
```

---

**End of Technical Design Document**
